/home/mila/o/ortizgas/CODE/milabench/benchmarks/vjepa/jepa/app/vjepa/utils.py:209: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if mixed_precision else None
/home/mila/o/ortizgas/CODE/milabench/benchmarks/vjepa/main.py:463: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with acc.amp.autocast(dtype=dtype, enabled=mixed_precision):
/home/mila/o/ortizgas/env/cp310/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/network/scratch/o/ortizgas/data/milabench/venv/torch/bin/voir", line 8, in <module>
[rank0]:     sys.exit(main())
[rank0]:   File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/voir/cli.py", line 128, in main
[rank0]:     ov(sys.argv[1:] if argv is None else argv)
[rank0]:   File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/voir/phase.py", line 331, in __call__
[rank0]:     self._run(*args, **kwargs)
[rank0]:   File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/voir/overseer.py", line 242, in _run
[rank0]:     set_value(func())
[rank0]:   File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/voir/scriptutils.py", line 37, in <lambda>
[rank0]:     return lambda: exec(mainsection, glb, glb)
[rank0]:   File "/home/mila/o/ortizgas/CODE/milabench/benchmarks/vjepa/main.py", line 656, in <module>
[rank0]:     main()
[rank0]:   File "/home/mila/o/ortizgas/CODE/milabench/benchmarks/vjepa/main.py", line 645, in main
[rank0]:     _main(params)
[rank0]:   File "/home/mila/o/ortizgas/CODE/milabench/benchmarks/vjepa/main.py", line 509, in _main
[rank0]:     (loss, loss_jepa, loss_reg, _new_lr, _new_wd, grad_stats, grad_stats_pred, optim_stats,), gpu_etime_ms = gpu_timer(train_step)
[rank0]:   File "/home/mila/o/ortizgas/CODE/milabench/benchmarks/vjepa/jepa/src/utils/logging.py", line 24, in gpu_timer
[rank0]:     result = closure()
[rank0]:   File "/home/mila/o/ortizgas/CODE/milabench/benchmarks/vjepa/main.py", line 474, in train_step
[rank0]:     scaler.scale(loss).backward()
[rank0]:   File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 147.75 MiB is free. Including non-PyTorch memory, this process has 79.26 GiB memory in use. Of the allocated memory 69.20 GiB is allocated by PyTorch, and 9.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
