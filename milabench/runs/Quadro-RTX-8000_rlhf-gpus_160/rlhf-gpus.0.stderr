/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-160m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/26 [00:00<?, ?it/s]                                        0%|          | 0/26 [00:29<?, ?it/s]  4%|▍         | 1/26 [00:29<12:17, 29.48s/it]                                                4%|▍         | 1/26 [00:50<12:17, 29.48s/it]  8%|▊         | 2/26 [00:50<09:53, 24.71s/it]                                                8%|▊         | 2/26 [01:12<09:53, 24.71s/it] 12%|█▏        | 3/26 [01:12<08:52, 23.16s/it]                                               12%|█▏        | 3/26 [01:33<08:52, 23.16s/it] 15%|█▌        | 4/26 [01:33<08:16, 22.56s/it]                                               15%|█▌        | 4/26 [01:55<08:16, 22.56s/it] 19%|█▉        | 5/26 [01:55<07:47, 22.27s/it]                                               19%|█▉        | 5/26 [02:17<07:47, 22.27s/it] 23%|██▎       | 6/26 [02:17<07:24, 22.21s/it]                                               23%|██▎       | 6/26 [02:39<07:24, 22.21s/it] 27%|██▋       | 7/26 [02:39<06:57, 21.99s/it]                                               27%|██▋       | 7/26 [03:00<06:57, 21.99s/it] 31%|███       | 8/26 [03:00<06:33, 21.85s/it]                                               31%|███       | 8/26 [03:22<06:33, 21.85s/it] 35%|███▍      | 9/26 [03:22<06:09, 21.75s/it]                                               35%|███▍      | 9/26 [03:44<06:09, 21.75s/it] 38%|███▊      | 10/26 [03:44<05:49, 21.81s/it]                                                38%|███▊      | 10/26 [04:06<05:49, 21.81s/it] 42%|████▏     | 11/26 [04:06<05:27, 21.80s/it]                                                42%|████▏     | 11/26 [04:27<05:27, 21.80s/it] 46%|████▌     | 12/26 [04:27<05:03, 21.71s/it]                                                46%|████▌     | 12/26 [04:48<05:03, 21.71s/it] 50%|█████     | 13/26 [04:48<04:41, 21.62s/it]                                                50%|█████     | 13/26 [05:10<04:41, 21.62s/it] 54%|█████▍    | 14/26 [05:10<04:19, 21.65s/it]                                                54%|█████▍    | 14/26 [05:32<04:19, 21.65s/it] 58%|█████▊    | 15/26 [05:32<03:59, 21.73s/it]                                                58%|█████▊    | 15/26 [05:53<03:59, 21.73s/it] 62%|██████▏   | 16/26 [05:53<03:36, 21.64s/it]                                                62%|██████▏   | 16/26 [06:15<03:36, 21.64s/it] 65%|██████▌   | 17/26 [06:15<03:15, 21.73s/it]                                                65%|██████▌   | 17/26 [06:37<03:15, 21.73s/it] 69%|██████▉   | 18/26 [06:37<02:53, 21.68s/it]                                                69%|██████▉   | 18/26 [06:59<02:53, 21.68s/it] 73%|███████▎  | 19/26 [06:59<02:32, 21.76s/it]                                                73%|███████▎  | 19/26 [07:21<02:32, 21.76s/it] 77%|███████▋  | 20/26 [07:21<02:10, 21.79s/it]                                                77%|███████▋  | 20/26 [07:43<02:10, 21.79s/it] 81%|████████  | 21/26 [07:43<01:48, 21.78s/it]                                                81%|████████  | 21/26 [08:04<01:48, 21.78s/it] 85%|████████▍ | 22/26 [08:04<01:27, 21.81s/it]                                                85%|████████▍ | 22/26 [08:26<01:27, 21.81s/it] 88%|████████▊ | 23/26 [08:26<01:05, 21.75s/it]                                                88%|████████▊ | 23/26 [08:48<01:05, 21.75s/it] 92%|█████████▏| 24/26 [08:48<00:43, 21.69s/it]                                                92%|█████████▏| 24/26 [09:10<00:43, 21.69s/it] 96%|█████████▌| 25/26 [09:10<00:21, 21.77s/it]W1211 07:31:38.019638 139713464034432 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGTERM death signal, shutting down workers
W1211 07:31:38.020094 139713464034432 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 314518 closing signal SIGTERM
W1211 07:31:38.020809 139713464034432 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 314519 closing signal SIGTERM
W1211 07:31:38.021138 139713464034432 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 314520 closing signal SIGTERM
W1211 07:31:38.021925 139713464034432 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 314521 closing signal SIGTERM
Traceback (most recent call last):
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1165, in launch_command
    multi_gpu_launcher(args)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/accelerate/commands/launch.py", line 799, in multi_gpu_launcher
    distrib_run.run(args)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 314438 got signal: 15
