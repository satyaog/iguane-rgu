Error.  nthreads cannot be larger than environment variable "NUMEXPR_MAX_THREADS" (64)Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  3.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.05s/it]
Traceback (most recent call last):
  File "/home/mila/o/ortizgas/CODE/milabench/benchmarks/llava/main.py", line 143, in <module>
    main()
  File "/home/mila/o/ortizgas/CODE/milabench/benchmarks/llava/main.py", line 67, in main
    processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py", line 321, in from_pretrained
    return processor_class.from_pretrained(
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/processing_utils.py", line 892, in from_pretrained
    args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/processing_utils.py", line 938, in _get_arguments_from_pretrained
    args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 897, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2271, in from_pretrained
    return cls._from_pretrained(
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2505, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 157, in __init__
    super().__init__(
  File "/network/scratch/o/ortizgas/data/milabench/venv/torch/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 115, in __init__
    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
Exception: data did not match any variant of untagged enum ModelWrapper at line 277156 column 3
